{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preparing the data\n",
    "import pandas as pd\n",
    "from langsmith import Client, evaluate, evaluate_existing\n",
    "from langsmith.schemas import Example, Run\n",
    "from swebench.harness.run_evaluation import run_instances\n",
    "import resource\n",
    "import docker\n",
    "from swebench.harness.docker_utils import list_images, clean_images\n",
    "from swebench.harness.docker_build import build_env_images\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load data from Hugging Face\n",
    "splits = {\n",
    "    'dev': 'data/dev-00000-of-00001.parquet',\n",
    "    'test': 'data/test-00000-of-00001.parquet',\n",
    "    'train': 'data/train-00000-of-00001.parquet'\n",
    "}\n",
    "df = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench/\" + splits[\"dev\"])\n",
    "\n",
    "# Fix version column formatting\n",
    "df['version'] = df['version'].apply(lambda x: f\"version:{x}\")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"./../SWE-bench.csv\", index=False)\n",
    "\n",
    "# Create LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Upload to LangSmith programmatically\n",
    "dataset = client.upload_csv(\n",
    "    csv_file=\"./../SWE-bench-dev.csv\",\n",
    "    input_keys=list(df.columns),\n",
    "    output_keys=[],\n",
    "    name=\"swe-bench-programatic-upload\",\n",
    "    description=\"SWE-bench dataset\",\n",
    "    data_type=\"kv\"\n",
    ")\n",
    "\n",
    "# Constants\n",
    "RUN_EVALUATION_LOG_DIR = Path(\"logs/run_evaluation\")\n",
    "LANGSMITH_EVALUATION_DIR = './langsmith_feedback/feedback.json'\n",
    "\n",
    "# Prediction function\n",
    "def predict(inputs: dict):\n",
    "    return {\n",
    "        \"instance_id\": inputs['instance_id'],\n",
    "        \"model_patch\": \"None\",\n",
    "        \"model_name_or_path\": \"test-model\"\n",
    "    }\n",
    "\n",
    "# Run initial evaluation\n",
    "result = evaluate(\n",
    "    predict,\n",
    "    data=client.list_examples(dataset_id=\"a9bffcdf-1dfe-4aef-8805-8806f0110067\", splits=[\"test\"]),\n",
    ")\n",
    "\n",
    "def convert_runs_to_langsmith_feedback(\n",
    "        predictions: dict,\n",
    "        full_dataset: list,\n",
    "        run_id: str\n",
    "    ) -> float:\n",
    "    \"\"\"Convert logs from docker containers into LangSmith feedback.\"\"\"\n",
    "    feedback_for_all_instances = {}\n",
    "\n",
    "    for instance in full_dataset:\n",
    "        feedback_for_instance = []\n",
    "        instance_id = instance['instance_id']\n",
    "        prediction = predictions[instance_id]\n",
    "        \n",
    "        if prediction.get(\"model_patch\", None) in [\"\", None]:\n",
    "            feedback_for_all_instances[prediction['run_id']] = [\n",
    "                {\"key\": \"non-empty-patch\", \"score\": 0},\n",
    "                {\"key\": \"completed-patch\", \"score\": 0},\n",
    "                {\"key\": \"resolved-patch\", \"score\": 0}\n",
    "            ]\n",
    "            continue\n",
    "            \n",
    "        feedback_for_instance.append({\"key\": \"non-empty-patch\", \"score\": 1})\n",
    "        report_file = (\n",
    "            RUN_EVALUATION_LOG_DIR\n",
    "            / run_id\n",
    "            / prediction[\"model_name_or_path\"].replace(\"/\", \"__\")\n",
    "            / prediction['instance_id']\n",
    "            / \"report.json\"\n",
    "        )\n",
    "        \n",
    "        if report_file.exists():\n",
    "            feedback_for_instance.append({\"key\": \"completed-patch\", \"score\": 1})\n",
    "            report = json.loads(report_file.read_text())\n",
    "            if report[instance_id][\"resolved\"]:\n",
    "                feedback_for_instance.append({\"key\": \"resolved-patch\", \"score\": 1})\n",
    "            else:\n",
    "                feedback_for_instance.append({\"key\": \"resolved-patch\", \"score\": 0})\n",
    "        else:\n",
    "            feedback_for_instance += [\n",
    "                {\"key\": \"completed-patch\", \"score\": 0},\n",
    "                {\"key\": \"resolved-patch\", \"score\": 0}\n",
    "            ]\n",
    "            \n",
    "        feedback_for_all_instances[prediction['run_id']] = feedback_for_instance\n",
    "\n",
    "    os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'w') as json_file:\n",
    "        json.dump(feedback_for_all_instances, json_file)\n",
    "\n",
    "def evaluate_predictions(\n",
    "        dataset: list,\n",
    "        predictions: list,\n",
    "        max_workers: int,\n",
    "        force_rebuild: bool,\n",
    "        cache_level: str,\n",
    "        clean: bool,\n",
    "        open_file_limit: int,\n",
    "        run_id: str,\n",
    "        timeout: int,\n",
    "    ):\n",
    "    \"\"\"Run evaluation harness for the given dataset and predictions.\"\"\"\n",
    "    assert len(run_id) > 0, \"Run ID must be provided\"\n",
    "    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n",
    "    client = docker.from_env()\n",
    "\n",
    "    existing_images = list_images(client)\n",
    "    print(f\"Running {len(dataset)} unevaluated instances...\")\n",
    "    \n",
    "    build_env_images(client, dataset, force_rebuild, max_workers)\n",
    "    run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)\n",
    "\n",
    "    clean_images(client, existing_images, cache_level, clean)\n",
    "    convert_runs_to_langsmith_feedback(predictions, dataset, run_id)\n",
    "\n",
    "# Prepare data for evaluation\n",
    "dataset = []\n",
    "predictions = {}\n",
    "for res in result:\n",
    "    predictions[res['run'].outputs['instance_id']] = {\n",
    "        **res['run'].outputs,\n",
    "        **{\"run_id\": str(res['run'].id)}\n",
    "    }\n",
    "    dataset.append(res['run'].inputs['inputs'])\n",
    "    \n",
    "for d in dataset:\n",
    "    d['version'] = d['version'].split(\":\")[1]\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_predictions(\n",
    "    dataset,\n",
    "    predictions,\n",
    "    max_workers=8,\n",
    "    force_rebuild=False,\n",
    "    cache_level=\"env\",\n",
    "    clean=False,\n",
    "    open_file_limit=4096,\n",
    "    run_id=\"test\",\n",
    "    timeout=1_800\n",
    ")\n",
    "\n",
    "# Create evaluator function\n",
    "def swe_bench_evaluator(run: Run, example: Example):\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'r') as json_file:\n",
    "        langsmith_eval = json.load(json_file)\n",
    "    return {\"results\": langsmith_eval[str(run.id)]}\n",
    "\n",
    "# Send evaluation to LangSmith\n",
    "experiment_name = result.experiment_name\n",
    "evaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
