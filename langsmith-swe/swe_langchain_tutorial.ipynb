{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Print debugging information\n",
    "# print(f\"Python path: {sys.executable}\")\n",
    "# print(f\"Python version: {sys.version}\")\n",
    "# print(f\"sys.path: {sys.path}\")\n",
    "\n",
    "# try:\n",
    "#     from langchain_openai import ChatOpenAI\n",
    "#     print(\"Successfully imported ChatOpenAI\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"Import error: {e}\")\n",
    "#     print(\"Try running: pip install langchain-openai\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "# # Initialize the ChatOpenAI instance with your API key\n",
    "# llm = ChatOpenAI(\n",
    "#     api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "# )\n",
    "\n",
    "# # Test the chat\n",
    "# response = llm.invoke(\"Hello, world!\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dataset to pandas DataFrame to allow item assignment\n",
    "# df = df.to_pandas()\n",
    "def make_dataset_csv():\n",
    "    import pandas as pd\n",
    "\n",
    "    # splits = {'dev': 'data/dev-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'train': 'data/train-00000-of-00001.parquet'}\n",
    "    # df = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench/\" + splits[\"dev\"])\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds = load_dataset(\"princeton-nlp/SWE-bench\")\n",
    "    df = ds['dev']\n",
    "    # Convert to pandas DataFrame first\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(df)\n",
    "    df['version'] = df['version'].apply(lambda x: f\"version:{x}\")\n",
    "    df.to_csv(\"SWE-bench-dev.csv\",index=False)\n",
    "    return df\n",
    "\n",
    "def upload_dataset_programmatically(df):\n",
    "    dataset = client.upload_csv(\n",
    "        csv_file=\"SWE-bench-dev.csv\",\n",
    "        input_keys=list(df.columns),\n",
    "        output_keys=[],\n",
    "        name=\"swe-bench-programatic-upload\",\n",
    "        description=\"SWE-bench dataset\",\n",
    "        data_type=\"kv\"\n",
    "    )\n",
    "upload_dataset_programmatically(make_dataset_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "\n",
    "def predict(inputs: dict):\n",
    "    return {\"instance_id\":inputs['instance_id'],\"model_patch\":\"None\",\"model_name_or_path\":\"test-model\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'best-uncle-91' at:\n",
      "https://smith.langchain.com/o/22d9505f-5add-45d4-84e0-97f2aa494e0d/datasets/20bfdc4c-5515-4391-909c-9ba463a23817/compare?selectedSessions=cbe445ed-a2b8-4f18-bb2f-0dfa43fd6dfb\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:01, 157.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = evaluate(\n",
    "    predict,\n",
    "    data=client.list_examples(dataset_id=\"20bfdc4c-5515-4391-909c-9ba463a23817\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('evaluation_results.json', 'w') as f:\n",
    "#     json.dump(dict(result.to_pandas()), f, indent=2)\n",
    "\n",
    "# dict(result.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ExperimentResults in module langsmith.evaluation._runner:\n",
      "\n",
      "class ExperimentResults(builtins.object)\n",
      " |  ExperimentResults(experiment_manager: '_ExperimentManager', blocking: 'bool' = True)\n",
      " |  \n",
      " |  Represents the results of an evaluate() call.\n",
      " |  \n",
      " |  This class provides an iterator interface to iterate over the experiment results\n",
      " |  as they become available. It also provides methods to access the experiment name,\n",
      " |  the number of results, and to wait for the results to be processed.\n",
      " |  \n",
      " |  Methods:\n",
      " |      experiment_name() -> str: Returns the name of the experiment.\n",
      " |      wait() -> None: Waits for the experiment data to be processed.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, experiment_manager: '_ExperimentManager', blocking: 'bool' = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> 'Iterator[ExperimentResultRow]'\n",
      " |  \n",
      " |  __len__(self) -> 'int'\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  to_pandas(self, start: 'Optional[int]' = 0, end: 'Optional[int]' = None) -> 'DataFrame'\n",
      " |  \n",
      " |  wait(self) -> 'None'\n",
      " |      Wait for the evaluation runner to complete.\n",
      " |      \n",
      " |      This method blocks the current thread until the evaluation runner has\n",
      " |      finished its execution.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  experiment_name\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('evaluation_results.json', 'w') as f:\n",
    "#     json.dump(result.dict(), f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swebench.harness.run_evaluation import run_instances\n",
    "import resource\n",
    "import docker\n",
    "from swebench.harness.docker_utils import list_images, clean_images\n",
    "from swebench.harness.docker_build import build_env_images\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "RUN_EVALUATION_LOG_DIR = Path(\"logs/run_evaluation\")\n",
    "LANGSMITH_EVALUATION_DIR = './langsmith_feedback/feedback.json'\n",
    "\n",
    "def convert_runs_to_langsmith_feedback(\n",
    "        predictions: dict,\n",
    "        full_dataset: list,\n",
    "        run_id: str\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Convert logs from docker containers into LangSmith feedback.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): Predictions dict generated by the model\n",
    "        full_dataset (list): List of all instances\n",
    "        run_id (str): Run ID\n",
    "    \"\"\"\n",
    "    feedback_for_all_instances = {}\n",
    "\n",
    "    for instance in full_dataset:\n",
    "        feedback_for_instance = []\n",
    "        instance_id = instance['instance_id']\n",
    "        prediction = predictions[instance_id]\n",
    "        if prediction.get(\"model_patch\", None) in [\"\", None]:\n",
    "            # Prediction returned an empty patch\n",
    "            feedback_for_all_instances[prediction['run_id']] = [{\"key\":\"non-empty-patch\",\"score\":0},\n",
    "                                                                {\"key\":\"completed-patch\",\"score\":0},\n",
    "                                                                {\"key\":\"resolved-patch\",\"score\":0}]\n",
    "            continue\n",
    "        feedback_for_instance.append({\"key\":\"non-empty-patch\",\"score\":1})\n",
    "        report_file = (\n",
    "            RUN_EVALUATION_LOG_DIR\n",
    "            / run_id\n",
    "            / prediction[\"model_name_or_path\"].replace(\"/\", \"__\")\n",
    "            / prediction['instance_id']\n",
    "            / \"report.json\"\n",
    "        )\n",
    "        if report_file.exists():\n",
    "            # If report file exists, then the instance has been run\n",
    "            feedback_for_instance.append({\"key\":\"completed-patch\",\"score\":1})\n",
    "            report = json.loads(report_file.read_text())\n",
    "            # Check if instance actually resolved the PR\n",
    "            if report[instance_id][\"resolved\"]:\n",
    "                feedback_for_instance.append({\"key\":\"resolved-patch\",\"score\":1})\n",
    "            else:\n",
    "                feedback_for_instance.append({\"key\":\"resolved-patch\",\"score\":0})\n",
    "        else:\n",
    "            # The instance did not run successfully\n",
    "            feedback_for_instance += [{\"key\":\"completed-patch\",\"score\":0},{\"key\":\"resolved-patch\",\"score\":0}]\n",
    "        feedback_for_all_instances[prediction['run_id']] = feedback_for_instance\n",
    "\n",
    "    os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'w') as json_file:\n",
    "        json.dump(feedback_for_all_instances, json_file)\n",
    "\n",
    "def evaluate_predictions(\n",
    "        dataset: list,\n",
    "        predictions: list,\n",
    "        max_workers: int,\n",
    "        force_rebuild: bool,\n",
    "        cache_level: str,\n",
    "        clean: bool,\n",
    "        open_file_limit: int,\n",
    "        run_id: str,\n",
    "        timeout: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Run evaluation harness for the given dataset and predictions.\n",
    "    \"\"\"\n",
    "    # set open file limit\n",
    "    assert len(run_id) > 0, \"Run ID must be provided\"\n",
    "    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n",
    "    client = docker.from_env()\n",
    "\n",
    "    existing_images = list_images(client)\n",
    "    print(f\"Running {len(dataset)} unevaluated instances...\")\n",
    "\n",
    "    try:\n",
    "        # build environment images + run instances\n",
    "        build_env_images(client, dataset, force_rebuild, max_workers)\n",
    "        run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)\n",
    "    except Exception as e:\n",
    "        print(f\"Docker image access error: {e}\")\n",
    "        return\n",
    "    print(\"No docker image access error\")\n",
    "    # clean images + make final report\n",
    "    clean_images(client, existing_images, cache_level, clean)\n",
    "    # import pdb;pdb.set_trace()\n",
    "    convert_runs_to_langsmith_feedback(predictions,dataset,run_id)\n",
    "    # pdb.set_trace()\n",
    "    pass\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "predictions = {}\n",
    "for res in result:\n",
    "    predictions[res['run'].outputs['instance_id']] = {**res['run'].outputs,**{\"run_id\":str(res['run'].id)}}\n",
    "    dataset.append(res['run'].inputs['inputs'])\n",
    "for d in dataset:\n",
    "    d['version'] = d['version'].split(\":\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with existing credentials...\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!docker login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 unevaluated instances...\n",
      "Base image sweb.base.py.arm64:latest already exists, skipping build.\n",
      "Base images built successfully.\n",
      "Total environment images to build: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/Desktop/research/coordination_among_ai/SWE-bench/swebench/harness/docker_build.py\", line 155, in build_image\n",
      "    raise docker.errors.BuildError(\n",
      "docker.errors.BuildError: The command '/bin/sh -c /bin/bash -c \"source ~/.bashrc && /root/setup_env.sh\"' returned a non-zero code: 1\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/Desktop/research/coordination_among_ai/SWE-bench/swebench/harness/utils.py\", line 92, in run_threadpool\n",
      "    future.result()\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/Desktop/research/coordination_among_ai/SWE-bench/swebench/harness/docker_build.py\", line 161, in build_image\n",
      "    raise BuildImageError(image_name, str(e), logger) from e\n",
      "swebench.harness.docker_build.BuildImageError: Error building image sweb.env.py.arm64.f8f7816be1e30e5ea7547a:latest: The command '/bin/sh -c /bin/bash -c \"source ~/.bashrc && /root/setup_env.sh\"' returned a non-zero code: 1\n",
      "Check (logs/build_images/env/sweb.env.py.arm64.f8f7816be1e30e5ea7547a__latest/build_image.log) for more information.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/Desktop/research/coordination_among_ai/SWE-bench/swebench/harness/docker_build.py\", line 155, in build_image\n",
      "    raise docker.errors.BuildError(\n",
      "docker.errors.BuildError: The command '/bin/sh -c /bin/bash -c \"source ~/.bashrc && /root/setup_env.sh\"' returned a non-zero code: 1\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/Desktop/research/coordination_among_ai/SWE-bench/swebench/harness/utils.py\", line 92, in run_threadpool\n",
      "    future.result()\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/Desktop/research/coordination_among_ai/SWE-bench/swebench/harness/docker_build.py\", line 161, in build_image\n",
      "    raise BuildImageError(image_name, str(e), logger) from e\n",
      "swebench.harness.docker_build.BuildImageError: Error building image sweb.env.py.arm64.63ba17657a55525251ae86:latest: The command '/bin/sh -c /bin/bash -c \"source ~/.bashrc && /root/setup_env.sh\"' returned a non-zero code: 1\n",
      "Check (logs/build_images/env/sweb.env.py.arm64.63ba17657a55525251ae86__latest/build_image.log) for more information.\n",
      "  0%|          | 0/2 [00:26<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'swebench.harness.docker_build.BuildImageError'>: Error building image sweb.env.py.arm64.f8f7816be1e30e5ea7547a:latest: The command '/bin/sh -c /bin/bash -c \"source ~/.bashrc && /root/setup_env.sh\"' returned a non-zero code: 1\n",
      "Check (logs/build_images/env/sweb.env.py.arm64.f8f7816be1e30e5ea7547a__latest/build_image.log) for more information.\n",
      "<class 'swebench.harness.docker_build.BuildImageError'>: Error building image sweb.env.py.arm64.63ba17657a55525251ae86:latest: The command '/bin/sh -c /bin/bash -c \"source ~/.bashrc && /root/setup_env.sh\"' returned a non-zero code: 1\n",
      "Check (logs/build_images/env/sweb.env.py.arm64.63ba17657a55525251ae86__latest/build_image.log) for more information.\n",
      "2 environment images failed to build.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5 ran successfully, 0 failed: 100%|██████████| 5/5 [00:00<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error building image pylint-dev__astroid-1333: 404 Client Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=swebench%2Fsweb.eval.arm64.pylint-dev_1776_astroid-1333: Not Found (\"pull access denied for swebench/sweb.eval.arm64.pylint-dev_1776_astroid-1333, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\")\n",
      "Check (logs/run_evaluation/test/test-model/pylint-dev__astroid-1333/run_instance.log) for more information.\n",
      "Error building image pvlib__pvlib-python-1623: 404 Client Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=swebench%2Fsweb.eval.arm64.pvlib_1776_pvlib-python-1623: Not Found (\"pull access denied for swebench/sweb.eval.arm64.pvlib_1776_pvlib-python-1623, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\")\n",
      "Check (logs/run_evaluation/test/test-model/pvlib__pvlib-python-1623/run_instance.log) for more information.\n",
      "Error building image sqlfluff__sqlfluff-3411: 404 Client Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=swebench%2Fsweb.eval.arm64.sqlfluff_1776_sqlfluff-3411: Not Found (\"pull access denied for swebench/sweb.eval.arm64.sqlfluff_1776_sqlfluff-3411, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\")\n",
      "Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-3411/run_instance.log) for more information.\n",
      "Error building image sqlfluff__sqlfluff-2998: 404 Client Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=swebench%2Fsweb.eval.arm64.sqlfluff_1776_sqlfluff-2998: Not Found (\"pull access denied for swebench/sweb.eval.arm64.sqlfluff_1776_sqlfluff-2998, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\")\n",
      "Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-2998/run_instance.log) for more information.\n",
      "Error building image pvlib__pvlib-python-1707: 404 Client Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=swebench%2Fsweb.eval.arm64.pvlib_1776_pvlib-python-1707: Not Found (\"pull access denied for swebench/sweb.eval.arm64.pvlib_1776_pvlib-python-1707, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\")\n",
      "Check (logs/run_evaluation/test/test-model/pvlib__pvlib-python-1707/run_instance.log) for more information.\n",
      "All instances run.\n",
      "No docker image access error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning cached images...\n",
      "Removed 0 images.\n",
      "> \u001b[0;32m/var/folders/g_/12fwrqfs2lgcf9rnql4m440dx849vl/T/ipykernel_46110/3502636115.py\u001b[0m(97)\u001b[0;36mevaluate_predictions\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     95 \u001b[0;31m    \u001b[0mclean_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 97 \u001b[0;31m    \u001b[0mconvert_runs_to_langsmith_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     99 \u001b[0;31m    \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/var/folders/g_/12fwrqfs2lgcf9rnql4m440dx849vl/T/ipykernel_46110/3502636115.py\u001b[0m(99)\u001b[0;36mevaluate_predictions\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     97 \u001b[0;31m    \u001b[0mconvert_runs_to_langsmith_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 99 \u001b[0;31m    \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    100 \u001b[0;31m    \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    101 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(dataset[:5],predictions,max_workers=8,force_rebuild=False,cache_level=\"env\",clean=False \\\n",
    "                     ,open_file_limit=4096,run_id=\"test\",timeout=1_800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'best-uncle-91' at:\n",
      "https://smith.langchain.com/o/22d9505f-5add-45d4-84e0-97f2aa494e0d/datasets/20bfdc4c-5515-4391-909c-9ba463a23817/compare?selectedSessions=cbe445ed-a2b8-4f18-bb2f-0dfa43fd6dfb\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2025-02-11 22:22:44,895 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6c7690b6-7fb5-4cc3-8775-d790f1e4b1cb: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:44,911 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7f269e1e-1600-4a35-a37e-4259a23e92d3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:44,927 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 553d7768-b269-4c3a-b2e9-03cc794309d5: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:44,944 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 638ffedf-f975-4ed0-97cd-71bbc0ae108e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:44,959 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d9269edb-d960-4117-b306-e5cced0884b8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:44,977 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 4ea657b2-5f41-4adb-9571-003e96e20efb: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "6it [00:00, 59.69it/s]2025-02-11 22:22:44,993 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e751f9d1-5172-401d-b1e9-46d3711faac3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,011 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cdf0c269-12bc-4594-8869-73f474c08661: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,029 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a3357ea5-b8d0-4682-999a-1f866c20dbc0: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,048 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 305ef12b-e529-4932-bb53-e6d55d448044: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,065 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 542b9197-afc5-4e57-9402-de81ef1b7669: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,083 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 59b0f850-2cf3-4cb1-a922-b52e2b9a3c1b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "12it [00:00, 45.56it/s]2025-02-11 22:22:45,154 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e1c50c86-c75a-41e1-9cb9-2a32d475e89a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,170 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 59214659-fde3-41ad-8d93-5e199233d97b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,186 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run be2e5a79-d999-42a0-918a-a779908eada8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,203 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 87a84772-31c4-4537-a475-1847aa18f2ff: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,219 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a55cc44a-f725-4646-aadc-8e3c43ec294f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,234 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 70c7b6da-5a00-4448-b92e-63b242c8fd7a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "18it [00:00, 50.48it/s]2025-02-11 22:22:45,252 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 0908d8a5-c46b-47a9-ab72-0b7ee83ee326: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,268 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9bd5b1e6-a13b-4181-85d0-06095dc367a0: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,285 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 21a7b5b9-7ceb-459b-8368-83f5e170e470: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,301 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f5cd06fa-310f-4db8-b536-5e74fc6fbcd4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,318 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f88545c4-e81d-47e7-bf23-5c2757068a71: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,335 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 142ac269-4e32-4ff0-b8e0-ff9a9439c22f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "24it [00:00, 53.64it/s]2025-02-11 22:22:45,352 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 0901bde7-68f6-4ed1-9433-fc83dc35878a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,369 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 444a5195-7614-41f9-b325-77b818b84d97: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,385 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c2b93dcd-8b1d-4767-baaa-7d0c3583b135: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,403 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 493c4f7e-7135-4749-b876-3d233592c5d2: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,418 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 91755c7a-06ac-4107-9e04-0fc9943f7874: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,435 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9e29b118-13ae-4e49-b48e-2dfd42f12ad8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,451 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f92f0cb0-d117-4054-b999-0ad391daefcd: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "31it [00:00, 56.08it/s]2025-02-11 22:22:45,469 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 0f65188a-5a0a-419e-ac6a-c739fae989ce: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,485 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 50a131b8-7f0d-4175-8511-4112fea9695e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,500 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 920f6dab-c37c-43fa-a4f7-7b829f71dbbe: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,517 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ace05a4d-c749-4558-9bad-afaf107d2845: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,533 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ecf51d43-058b-4bec-b8e1-d5d5a217bc2e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,548 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 826880f6-a0a7-48ef-b8fd-bef026aa7d67: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,565 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c350292e-2f96-4a6e-ad52-88524d5081e6: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "38it [00:00, 57.90it/s]2025-02-11 22:22:45,583 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3408927c-c704-46b6-921e-386f8f18c1aa: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,601 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d235f6c5-3c45-48b9-9d08-cf8a4ce58776: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,618 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f8c39455-0002-487c-8506-872505c53650: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,633 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 05da421d-b161-4a71-982b-8de71c3ab1a1: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,649 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6630caba-af85-4462-aca5-e95e9f9e1d28: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,666 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 56ac6db2-b604-41c4-9589-b7c2a51ce097: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "44it [00:00, 58.39it/s]2025-02-11 22:22:45,683 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9263ca32-3a7c-42d0-b5e6-095eefbf9695: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,700 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9dec22ef-bef8-46a7-bfeb-c73ab09489be: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,716 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 313920a4-9ba9-4e4c-9b40-36ba604aada7: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,732 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 787c9799-b87a-48cc-b3ca-7235e560b8e8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,748 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 4da6a351-51a0-43df-ad66-9c55fb4f5b8b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,764 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 58e02ac3-6e40-489d-85f3-d9333434cf5c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,780 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7aaded43-3c75-4c02-8f5e-c127d4b08071: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "51it [00:00, 59.38it/s]2025-02-11 22:22:45,797 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d72facbb-a68a-4799-8add-150edacce0a7: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,814 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b5409127-0788-4240-9d7f-bad6413fe02f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,830 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 2e615d57-98a3-4cdf-a173-bbe45b86b3a4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,847 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9be355a6-0860-480c-8d12-abe824e6f5a5: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,863 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 68adb5be-84f0-425c-95dc-58ad4d68af10: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,879 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c65c2cb5-5cf6-4fd3-a7da-9dd38e3ffe00: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,894 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b29bcc17-81d6-4cb9-9a8b-c60f56ff278f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "58it [00:01, 60.02it/s]2025-02-11 22:22:45,912 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7422546e-112f-4798-9bd4-f7a9c5710158: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,928 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 8505293f-f014-42f6-94f8-c87a8e4eeadb: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,945 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 27d6af1a-220c-4738-bfe0-6e9d954607a7: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,960 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9dcb978a-5550-4a9a-a899-5a817cd66ddb: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,975 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cf9fc7d0-84e9-488f-ab5d-2875453c5cf5: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:45,991 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b86825c0-5f9c-4b36-99f1-c27373a8d1b2: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,007 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a6a0ec4b-c9e1-41b6-ba5b-20e2e25abda3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "65it [00:01, 60.65it/s]2025-02-11 22:22:46,024 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3c0ae26e-375a-4ff9-b5ee-5f1d382f1471: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,041 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3907341e-68aa-4c02-a0ec-42a851ebbebf: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,057 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 2462bc72-19e2-4528-866f-358939239e22: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,075 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run dba90d7b-95b1-476d-b2dd-03a4e9422307: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,092 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b68fca37-26e0-499f-8c40-2c277f852b83: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,109 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a23f2d15-e7ff-4650-aa99-12c199b9a505: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,124 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run db8fd791-a6ae-4191-a6af-8e5370a34706: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "72it [00:01, 60.33it/s]2025-02-11 22:22:46,141 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 75a25ca8-bf1a-495d-abb4-dcb777d912f4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,156 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 59770e52-7fa0-441f-b013-7e94776ad414: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,172 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f20d8804-2c7e-4647-83c9-925bfe34a1ba: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,188 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ab4fdc57-dae3-4fa2-9419-4b16be02dff4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,204 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 39a6050c-af90-409d-9f98-b3e688c83119: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,221 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d0efdef3-093e-47cc-b085-2e3718cdfa1e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,237 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 83ca32cc-4a5a-4290-bc4d-d4028781eb5c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "79it [00:01, 60.90it/s]2025-02-11 22:22:46,254 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 91c548c0-a8df-4551-877a-a4d5cf8dd729: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,331 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9b6f7c11-2d55-4681-a4d2-13af0b2c6a69: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,350 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3674a8b2-732c-4de3-9810-5a40d3d9e366: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,368 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7f02732e-e905-4795-92b1-424dabf8e3f0: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,387 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 5ce7bdc0-cba5-486c-b328-39c0864c2048: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,405 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b6e812e3-eb6d-437d-a7a3-14e81e3c2e31: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,422 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run eaca63a9-6c8f-4c41-a4f6-d84b6d647679: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "86it [00:01, 51.17it/s]2025-02-11 22:22:46,442 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 85c9c157-85d7-44d1-b2b4-edb21568d773: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,462 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 73a0f5f2-d397-461c-87ed-4fa012da8469: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,481 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cc1054b7-81eb-49b2-99c8-575d6adacd10: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,498 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 344fe66d-5865-4086-9554-7303c7338ea8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,515 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6d075111-aa23-4090-82a3-9de78fa43a8f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,532 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 93beb737-3fca-4bf0-8db7-2e47f280791c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "92it [00:01, 52.09it/s]2025-02-11 22:22:46,552 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 5607e14e-5166-4c07-9800-e0cde20cfbaa: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,568 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b7edadc1-e71b-4712-8554-691bd0f3da57: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,585 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e0b8ea26-c027-4645-9205-f3d681bb9fcd: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,602 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a1d993c7-1802-4183-bc7e-fcdd70a7a742: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,618 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 07ee9225-9d65-4310-a8c5-c2bf1564ced8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,634 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d74d8580-02ea-4ec1-a3fd-838c59ed3bf0: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "98it [00:01, 53.83it/s]2025-02-11 22:22:46,655 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e3f2d003-263c-488e-816e-cd0405b180ee: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,673 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7eac0b86-099a-48b4-8b58-d2f69aaafb28: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,689 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6dd38b8b-2e89-420d-9a2f-29f8a63e9489: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,708 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 056ae329-e4bc-43a6-9295-ec57432f01c2: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,725 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c93fc05e-d401-4917-86e7-d47fbdea8c1f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,742 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b01c4581-2851-43de-b381-47e56b181677: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "104it [00:01, 54.45it/s]2025-02-11 22:22:46,759 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 39bb779a-e8be-4ef4-ba30-bf6a0d7dcd3a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,775 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e64aef22-d795-4e27-84b2-db083f645e68: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,793 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 8396ec38-f18d-42cc-a998-df711bd06390: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,811 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 484150f9-2444-40e0-86a5-e0313f66eb04: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,828 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 52c1dafb-3412-4662-98e1-4f738ebd5f63: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,845 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c09dd979-53b0-441e-bee6-0932c538a9bf: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "110it [00:01, 54.82it/s]2025-02-11 22:22:46,867 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9d79a4c0-e8ee-4d50-b017-cc7be55d7836: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,884 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3a6c29b9-70e6-4df2-a6f8-fb7819a986d6: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,900 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 39ed9c42-6728-4278-98cf-80b03b21505f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,917 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a6cfbac7-7790-42e8-bd3d-ddf10b39ea69: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,935 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ca7af54b-7401-4f7b-af75-2abbb139886d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,953 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c0e5e4f1-1e26-4acb-bf40-f23d32531427: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "116it [00:02, 55.71it/s]2025-02-11 22:22:46,970 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f1d34f0f-6fdd-4522-bbb5-f19d7d9c30c5: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:46,988 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 467bee63-05bc-462d-91c4-faae0cb5a60e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,005 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run fd25d2e3-9b6b-4dc1-b07d-47ce59bd31f7: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,023 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ab20b685-9efc-48ab-b3ab-cd25d5f6e82c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,039 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ffb74e8d-a9a2-4440-a156-c5e479cff044: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,056 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 02ad01d2-4b39-4e64-8ff8-28f985d9504f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "122it [00:02, 56.22it/s]2025-02-11 22:22:47,075 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 19e33fe2-6d8c-4d45-ace9-8b6fc2b7521b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,098 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cd96d722-f740-4ec6-8b70-1ebef202cc35: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,174 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run bb76cb73-8ccf-4d3f-b2af-90f1b4c9f9cb: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,199 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 60c5005d-5e96-4c53-bc99-ec68524be7a8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,216 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 202d5a21-b16f-45e7-b940-e86c28b99d03: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,232 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run bcd07792-4d2a-4a17-a01f-f2bce86f9c6d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "128it [00:02, 47.22it/s]2025-02-11 22:22:47,249 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 767334b9-3d1f-4389-8612-7b232be92eee: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,268 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 729305d9-f451-4772-8196-88af64c6d6d0: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,285 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7276a466-245f-4115-8bc4-42c3f13dcc46: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,303 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 7d1c3885-fa36-499f-93a2-a5be7e6780af: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,319 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c3c689a0-ed9a-4b7b-b4b0-876c3e0242c7: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,340 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 03a5adbe-1710-44f7-975c-77637170488c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "134it [00:02, 49.48it/s]2025-02-11 22:22:47,357 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cac1be55-71b5-4839-8ff2-9046d85ccae7: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,375 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 24e10047-be66-44e6-847c-19dc20401a7a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,394 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 659e3b28-ad85-4521-ad6c-04acb1eec35a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,412 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 45ff34db-6070-4632-bd10-0ecd9244c51a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,429 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c18b9ce6-1afc-4b77-b4f2-04226c97deda: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,446 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f21f045d-6660-4a22-b5ba-541a683e5dd3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "140it [00:02, 51.36it/s]2025-02-11 22:22:47,464 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 60dc3878-392a-465f-a9dc-7bae746f395b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,486 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run aa5e00ee-7420-4832-b27d-50b48d23e4ed: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,505 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3a70de74-8f28-4201-b03e-e8a3ee58bee2: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,522 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 8a32ac2a-ad68-44c7-bada-97c4de585679: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,539 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run effbd8bf-eef1-437c-b6b2-f909fa0608be: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,559 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9f83edb4-b34d-4fc8-985c-06b576898288: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "146it [00:02, 51.98it/s]2025-02-11 22:22:47,578 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 0f5045d2-fe15-45e9-bf56-d0b36d0886a3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,599 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 40ff469e-b1e2-422e-9cd3-34d64ff374af: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,618 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 1347d0b8-8540-4c5b-b0a4-5485da5b3c86: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,637 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 86ac6b2f-6907-4ada-88ef-c5fc2a04476c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,656 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e4fdd803-f079-440a-aca3-cadd2724afef: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,678 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 73c5e552-c379-4c11-82a5-d47e2c9ad304: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "152it [00:02, 51.41it/s]2025-02-11 22:22:47,698 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b6cce525-b55d-451f-98e1-0d811b80f8d8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,720 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 460fe3cb-f94b-49de-9591-df3dacfd49ca: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,739 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c25d48d2-ca21-4207-b86f-808af21971f3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,757 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 496b53a8-a1e1-4711-a576-ae5ba30668aa: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,776 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b7424f42-5a1c-4ad1-b5cb-d311f6abc3b1: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,795 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f478db18-b641-4c1a-834a-e0a8a145f921: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "158it [00:02, 51.18it/s]2025-02-11 22:22:47,817 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 1de9d316-57df-4d17-9410-2fccb5709f2a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,893 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f74da486-8c67-48c8-91b6-3d81c7f8b350: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,922 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 8de96b9f-828a-44b9-a30d-19d4556dd180: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,940 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 46b8c324-57ce-42b5-bcb6-7f9da62ef2d5: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,957 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 627a8124-3ab3-472d-8e88-8d180adc0b18: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:47,975 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6ec9683a-0cc8-4eb8-9db5-5b79d997d37a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "164it [00:03, 44.28it/s]2025-02-11 22:22:47,993 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ff2c5b61-bffd-4652-91f7-06710a9c257e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,016 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 28f6af49-368f-4bed-8d48-f5bc3404dbfc: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,033 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run a4e99532-37e0-4697-ad8b-3d81d64dcc80: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,050 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cc2df879-0cb1-4ed3-a370-8b2916eedb05: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,071 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 87a23cf0-28d0-4a88-8e00-cd4ad1eb0a8a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,089 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 823c8eff-7150-4b6a-aab2-43b3639483f5: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "170it [00:03, 46.48it/s]2025-02-11 22:22:48,109 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 202e1abb-63af-49d8-a781-207e169d6644: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,127 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9fb9f5b9-57cc-4a0e-92d7-0fe0c4a44fdf: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,145 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run b85adae3-30a5-4170-808b-1be9bc94649c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,170 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 342f2f06-fda7-418b-bacf-908d05c78473: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,190 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f1946b6f-87f5-4a7b-ba24-3e2377f47b2a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "175it [00:03, 46.99it/s]2025-02-11 22:22:48,212 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 1f223120-e11e-4966-b349-860cae2449f6: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,233 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c71d577b-a8cb-4ce6-8550-c79529203b8e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,252 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 3ce157b5-c21f-45cd-b981-4421d104a412: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,273 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6bc6f8e1-92df-4eef-8880-ed5c5613bcac: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,294 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 5d58bf66-24a9-4880-b283-89a087c1aaea: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "180it [00:03, 47.47it/s]2025-02-11 22:22:48,315 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f116a015-7acb-46d1-9ad8-b6dd23948fb3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,346 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f6656a94-9ce9-4cdd-8c23-5e606078f236: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,362 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e03e640c-6f7c-434d-9544-604df51c4221: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,380 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 99794ee9-8571-4806-a2a5-33d9f354227a: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,397 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 8e4eb35c-69cc-4a98-9f67-66828fd57ee2: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "185it [00:03, 47.81it/s]2025-02-11 22:22:48,416 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 626f0e2c-a332-4d0b-a240-6de05a4b3ee4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,434 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 92e449c7-c8e3-460e-9247-ca057a779223: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,454 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run ed660091-0bbb-4d22-84db-5c2138dafb9f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,475 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 14011416-c5c0-4ff2-9329-9cf6989197d9: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,551 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 1576f514-7c75-4cd4-9ebe-3dbb0674f480: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "190it [00:03, 42.13it/s]2025-02-11 22:22:48,568 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9accf76e-66dc-4734-b839-d9f2c65108ff: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,586 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e71aa5dd-51f5-4daf-a903-92935139d2a1: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,603 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 956d6cfc-1740-4ff9-bd06-a81fdd700f92: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,621 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cc2e2f7a-348d-4d28-bc64-a41d9706db7d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,641 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e0930a38-4f41-4c04-a811-872949341a5b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,657 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 6546b112-9e2e-4cda-a83a-46d7ac0d5286: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "196it [00:03, 45.96it/s]2025-02-11 22:22:48,675 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 13f54aee-08f3-42b8-b450-f8edbf8ccfe4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,693 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 4b25dd06-ca97-4a4e-bf19-d2ed443f3b82: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,712 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 223e69af-2d99-4242-90c5-98d71c4ff660: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,729 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f8a664ee-2704-4fe5-a64e-33869006dff3: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,748 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 13594166-9152-4fb2-9787-3704e0da1c41: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,766 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9e5047f9-aca2-4863-b3d1-9ee4981b9b0d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "202it [00:03, 47.98it/s]2025-02-11 22:22:48,789 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 506b192e-71cd-478f-8a24-47ec09b16964: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,807 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run f4d20223-a57c-4987-b754-1a3995a20297: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,824 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c972ff70-3793-4750-abc0-748405431b0d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,843 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 0dcad959-a31f-4ec7-8228-4a077945275f: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,860 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 99e0b097-59af-4492-bbbd-a2be8dfa5d9c: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,882 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run cfd9b071-b602-4d3d-bdd7-3ccf24bd8f22: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "208it [00:04, 49.62it/s]2025-02-11 22:22:48,901 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 20245336-91e1-49cb-8bea-aea35f64f338: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,919 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9a6c8659-babe-433b-ae71-302b7f7ca6e8: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,938 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c2a42586-566a-4677-8c10-a0d72f00a9bc: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,956 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 80a4c31a-effa-476f-a9a5-4b602bd7b2ce: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,974 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run c6100b2e-fd2a-4825-9670-4a252a14c47d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:48,992 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 5d41b3e4-b021-42da-b1fe-50a9d97b0d1b: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "214it [00:04, 51.07it/s]2025-02-11 22:22:49,011 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9bd4dd3d-1f31-46af-9c60-d0df294d1cd4: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:49,086 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d2e1dc12-3795-4f54-b238-808fd5198d3d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:49,112 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run e23c35e7-8f32-447f-a0d8-c02f33493da9: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:49,130 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run d5cb23de-4fa0-49b4-b5bc-b4626da18723: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:49,147 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 9c31a9a2-b329-4715-b9da-3d648191b10d: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "2025-02-11 22:22:49,166 - langsmith.evaluation._runner - ERROR - Error running evaluator <DynamicRunEvaluator swe_bench_evaluator> on run 03747d13-7837-4b92-928a-7b33766afee1: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 336, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 293, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 271, in _coerce_evaluation_results\n",
      "    cp[\"results\"] = [\n",
      "                    ^\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 272, in <listcomp>\n",
      "    self._coerce_evaluation_result(r, source_run_id=source_run_id)\n",
      "  File \"/Users/matthew.khoriaty/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 253, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score' or categorical 'value'; got N\n",
      "225it [00:04, 51.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.repo</th>\n",
       "      <th>inputs.patch</th>\n",
       "      <th>inputs.version</th>\n",
       "      <th>inputs.created_at</th>\n",
       "      <th>inputs.hints_text</th>\n",
       "      <th>inputs.test_patch</th>\n",
       "      <th>inputs.base_commit</th>\n",
       "      <th>inputs.instance_id</th>\n",
       "      <th>inputs.FAIL_TO_PASS</th>\n",
       "      <th>inputs.PASS_TO_PASS</th>\n",
       "      <th>...</th>\n",
       "      <th>outputs.model_patch</th>\n",
       "      <th>outputs.model_name_or_path</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.swe_bench_evaluator</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "      <th>feedback.non-empty-patch</th>\n",
       "      <th>feedback.completed-patch</th>\n",
       "      <th>feedback.resolved-patch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pylint-dev/astroid</td>\n",
       "      <td>diff --git a/astroid/nodes/node_ng.py b/astroi...</td>\n",
       "      <td>version:2.10</td>\n",
       "      <td>2022-01-14T21:14:48Z</td>\n",
       "      <td>Actually, it seems a decorator is a red herrin...</td>\n",
       "      <td>diff --git a/tests/unittest_builder.py b/tests...</td>\n",
       "      <td>cfd9e74f7b4cbac08357cadec03c736501368afa</td>\n",
       "      <td>pylint-dev__astroid-1351</td>\n",
       "      <td>[\"tests/unittest_builder.py::FromToLineNoTest:...</td>\n",
       "      <td>[\"tests/unittest_builder.py::FromToLineNoTest:...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>ffd119a9-29f8-436a-8984-9e1ec12f37ee</td>\n",
       "      <td>6c7690b6-7fb5-4cc3-8775-d790f1e4b1cb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pydicom/pydicom</td>\n",
       "      <td>diff --git a/pydicom/filewriter.py b/pydicom/f...</td>\n",
       "      <td>version:1.4</td>\n",
       "      <td>2020-04-17T01:12:48Z</td>\n",
       "      <td></td>\n",
       "      <td>diff --git a/pydicom/tests/test_filereader.py ...</td>\n",
       "      <td>caf0db105ddf389ff5025937fd5f3aa1e61e85e7</td>\n",
       "      <td>pydicom__pydicom-1076</td>\n",
       "      <td>[\"pydicom/tests/test_filereader.py::TestReader...</td>\n",
       "      <td>[\"pydicom/tests/test_filereader.py::TestReader...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>ff7bed2b-a486-40dc-a0c3-3f64878ed455</td>\n",
       "      <td>7f269e1e-1600-4a35-a37e-4259a23e92d3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pylint-dev/astroid</td>\n",
       "      <td>diff --git a/astroid/brain/brain_datetime.py b...</td>\n",
       "      <td>version:3.0</td>\n",
       "      <td>2023-06-22T03:00:19Z</td>\n",
       "      <td></td>\n",
       "      <td>diff --git a/tests/brain/test_brain.py b/tests...</td>\n",
       "      <td>efb34f2b84c9f019ffceacef3448d8351563b6a2</td>\n",
       "      <td>pylint-dev__astroid-2219</td>\n",
       "      <td>[\"tests/brain/test_brain.py::CollectionsDequeT...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>ff382806-b03b-4316-9a92-d81ab6dd1a04</td>\n",
       "      <td>553d7768-b269-4c3a-b2e9-03cc794309d5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sqlfluff/sqlfluff</td>\n",
       "      <td>diff --git a/src/sqlfluff/rules/L039.py b/src/...</td>\n",
       "      <td>version:0.6</td>\n",
       "      <td>2021-10-22T18:23:33Z</td>\n",
       "      <td>Does running `sqlfluff fix` again correct the ...</td>\n",
       "      <td>diff --git a/test/rules/std_L003_L036_L039_com...</td>\n",
       "      <td>a1579a16b1d8913d9d7c7d12add374a290bcc78c</td>\n",
       "      <td>sqlfluff__sqlfluff-1733</td>\n",
       "      <td>[\"test/rules/std_L003_L036_L039_combo_test.py:...</td>\n",
       "      <td>[\"test/rules/std_L016_L36_combo_test.py::test_...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>ff29f60a-5881-43e5-b9ce-b55eec2aba8e</td>\n",
       "      <td>638ffedf-f975-4ed0-97cd-71bbc0ae108e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pvlib/pvlib-python</td>\n",
       "      <td>diff --git a/pvlib/ivtools/sdm.py b/pvlib/ivto...</td>\n",
       "      <td>version:0.9</td>\n",
       "      <td>2023-04-27T14:29:30Z</td>\n",
       "      <td></td>\n",
       "      <td>diff --git a/pvlib/tests/test_pvsystem.py b/pv...</td>\n",
       "      <td>30c62e368529df01faa609d6b38456a7b0db9b53</td>\n",
       "      <td>pvlib__pvlib-python-1719</td>\n",
       "      <td>[\"pvlib/tests/test_pvsystem.py::test_v_from_i[...</td>\n",
       "      <td>[\"pvlib/tests/test_pvsystem.py::test_PVSystem_...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>fe5f9897-72ef-4db9-a5cd-92373ab5469c</td>\n",
       "      <td>d9269edb-d960-4117-b306-e5cced0884b8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>pvlib/pvlib-python</td>\n",
       "      <td>diff --git a/pvlib/iam.py b/pvlib/iam.py\\n--- ...</td>\n",
       "      <td>version:0.9</td>\n",
       "      <td>2023-03-24T10:46:42Z</td>\n",
       "      <td></td>\n",
       "      <td>diff --git a/pvlib/tests/test_iam.py b/pvlib/t...</td>\n",
       "      <td>40e9e978c170bdde4eeee1547729417665dbc34c</td>\n",
       "      <td>pvlib__pvlib-python-1707</td>\n",
       "      <td>[\"pvlib/tests/test_iam.py::test_physical_n1_L0\"]</td>\n",
       "      <td>[\"pvlib/tests/test_iam.py::test_ashrae\", \"pvli...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0299893e-0c12-4564-ba45-833f024c2c91</td>\n",
       "      <td>cdd8a49a-7dc3-4f00-a7ce-d91c61d93735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>sqlfluff/sqlfluff</td>\n",
       "      <td>diff --git a/src/sqlfluff/rules/L027.py b/src/...</td>\n",
       "      <td>version:0.11</td>\n",
       "      <td>2022-04-04T20:29:42Z</td>\n",
       "      <td>This is tricky.\\r\\n\\r\\nBasicaly L026 works to ...</td>\n",
       "      <td>diff --git a/test/fixtures/rules/std_rule_case...</td>\n",
       "      <td>47c8bb29104761474e455ef2e6fdaa7a8cc20a56</td>\n",
       "      <td>sqlfluff__sqlfluff-2998</td>\n",
       "      <td>[\"test/rules/std_test.py::test__rules__std_fil...</td>\n",
       "      <td>[\"test/rules/std_test.py::test__rules__std_fil...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>029542a0-ea97-466d-81c4-81852ac72de2</td>\n",
       "      <td>23ba5b76-442c-4a3c-aa80-de5dd5c4c6a2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>pvlib/pvlib-python</td>\n",
       "      <td>diff --git a/docs/examples/adr-pvarray/plot_si...</td>\n",
       "      <td>version:0.8</td>\n",
       "      <td>2022-12-21T16:44:19Z</td>\n",
       "      <td></td>\n",
       "      <td>diff --git a/pvlib/tests/iotools/test_tmy.py b...</td>\n",
       "      <td>30c62e368529df01faa609d6b38456a7b0db9b53</td>\n",
       "      <td>pvlib__pvlib-python-1623</td>\n",
       "      <td>[\"pvlib/tests/iotools/test_tmy.py::test_read_t...</td>\n",
       "      <td>[\"pvlib/tests/iotools/test_tmy.py::test_read_t...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0241955e-a72c-4d1b-b2c3-5bc0a245f8d0</td>\n",
       "      <td>421e8e98-c37f-4e1a-949a-9ccae1bfaddd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>sqlfluff/sqlfluff</td>\n",
       "      <td>diff --git a/src/sqlfluff/cli/commands.py b/sr...</td>\n",
       "      <td>version:0.13</td>\n",
       "      <td>2022-05-30T18:41:48Z</td>\n",
       "      <td>Pending feedback from the admins, I'm happy to...</td>\n",
       "      <td>diff --git a/test/cli/commands_test.py b/test/...</td>\n",
       "      <td>e60272a859e37e335088ae79a7ad59ea771545a1</td>\n",
       "      <td>sqlfluff__sqlfluff-3411</td>\n",
       "      <td>[\"test/cli/commands_test.py::test__cli__comman...</td>\n",
       "      <td>[\"test/cli/commands_test.py::test__cli__comman...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0180d30b-84a6-4ae9-8a0c-4f60bcf40e9c</td>\n",
       "      <td>97a2e0f4-ffe4-44fc-90e0-be6273800ff9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>pylint-dev/astroid</td>\n",
       "      <td>diff --git a/astroid/modutils.py b/astroid/mod...</td>\n",
       "      <td>version:2.10</td>\n",
       "      <td>2022-01-08T19:36:45Z</td>\n",
       "      <td>I can't seem to reproduce this in my `virtuale...</td>\n",
       "      <td>diff --git a/tests/unittest_modutils.py b/test...</td>\n",
       "      <td>d2a5b3c7b1e203fec3c7ca73c30eb1785d3d4d0a</td>\n",
       "      <td>pylint-dev__astroid-1333</td>\n",
       "      <td>[\"tests/unittest_modutils.py::ModPathFromFileT...</td>\n",
       "      <td>[\"tests/unittest_modutils.py::ModuleFileTest::...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>test-model</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>00b33139-a242-4033-a52b-be3a55c6dc3e</td>\n",
       "      <td>9aa72797-5820-4563-be30-82c859f7ad64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults best-uncle-91>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate_existing\n",
    "from langsmith.schemas import Example, Run\n",
    "with open(LANGSMITH_EVALUATION_DIR, 'r') as json_file:\n",
    "    langsmith_eval = json.load(json_file)\n",
    "keyerrorcount = 0\n",
    "def swe_bench_evaluator(run: Run, example: Example):\n",
    "\n",
    "    return {\"results\": langsmith_eval.get(str(run.id), \"No results found, not run for this one\")}\n",
    "\n",
    "experiment_name = result.experiment_name\n",
    "evaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import evaluate_existing\n",
    "# from langsmith.schemas import Example, Run\n",
    "\n",
    "# def swe_bench_evaluator(run: Run, example: Example):\n",
    "#     with open(LANGSMITH_EVALUATION_DIR, 'r') as json_file:\n",
    "#         langsmith_eval = json.load(json_file)\n",
    "#     result = langsmith_eval.get(str(run.id))\n",
    "#     if result is None:\n",
    "#         # Log a warning or handle the missing result as needed.\n",
    "#         #print(f\"Warning: No evaluation result found for run id: {run.id}\")\n",
    "#         return {\"results\": None, \"error\": \"No result found for this run id.\"}\n",
    "#     return {\"results\": result}\n",
    "\n",
    "# experiment_name = result.experiment_name\n",
    "# evaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
