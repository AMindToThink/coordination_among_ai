This is from pvlib__pvlib-python-807 on deepseek-r1__SWE-bench_oracle__dev__shard-0__num_shards-225.jsonl

ChatCompletion(id='gen-1741146689-7bflZwAAMt4OpKhOeVEI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To address the issue of adding the Wavelet Variability Model (WVM) for spatial smoothing of irradiance, we\'ll create a new module `scaling.py` and integrate it into the pvlib package.\n\n### Changes:\n1. **New Module `scaling.py`**: Implements the WVM with functions for wavelet decomposition, spatial smoothing, and reconstruction.\n2. **Update `__init__.py`**: Exposes the new `wavelet_variability_model` function in the pvlib namespace.\n\n### Patch File:\n```patch\n--- a/pvlib/__init__.py\n+++ b/pvlib/__init__.py\n@@ -1,6 +1,7 @@\n from pvlib import (  # noqa: F401\n     atmosphere, bifacial, clearsky, iam, inverter, ivtools, location, modelchain,\n     pvsystem, singlediode, soiling, spectrum, temperature, tools, tracking)\n+from pvlib.scaling import wavelet_variability_model  # noqa: F401\n \n from pvlib._deprecation import deprecated  # isort:skip\n import pvlib.ivtools  # isort:skip\n--- /dev/null\n+++ b/pvlib/scaling.py\n@@ -0,0 +1,267 @@\n+"""\n+The scaling module contains functions for spatial and temporal scaling of\n+irradiance data, such as the Wavelet Variability Model (WVM).\n+"""\n+\n+import numpy as np\n+import pandas as pd\n+from scipy.spatial.distance import squareform, pdist\n+\n+\n+def wavelet_variability_model(clear_sky_index, dt, locations,\n+                              wavelet=\'haar\', max_level=None,\n+                              spatial_correlation=None):\n+    """\n+    Compute the spatially smoothed clear sky index using the Wavelet Variability\n+    Model (WVM) as described in [1]_ and [2]_.\n+\n+    Parameters\n+    ----------\n+    clear_sky_index : pandas.DataFrame or pandas.Series\n+        The clear sky index time series. If a DataFrame, each column represents\n+        a different location.\n+    dt : float\n+        Time step between consecutive data points in seconds.\n+    locations : pandas.DataFrame\n+        DataFrame containing the \'latitude\' and \'longitude\' columns for each\n+        location. The index should match the columns of `clear_sky_index` if it\n+        is a DataFrame.\n+    wavelet : str, optional\n+        Type of wavelet to use. Currently only \'haar\' is supported.\n+        Default is \'haar\'.\n+    max_level : int, optional\n+        Maximum level of wavelet decomposition. If None, the maximum possible\n+        level is determined based on the length of the data. Default is None.\n+    spatial_correlation : callable, optional\n+        Function that computes the spatial correlation matrix based on\n+        distances. If None, a default exponential decay function is used with\n+        a characteristic distance of 2 km. Default is None.\n+\n+    Returns\n+    -------\n+    smoothed_clear_sky_index : pandas.DataFrame or pandas.Series\n+        The spatially smoothed clear sky index.\n+\n+    References\n+    ----------\n+    .. [1] Lave, M., J. Kleissl, and J.S. Stein. A Wavelet-Based Variability\n+       Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n+       Energy, vol. 4, no. 2, pp. 501-509, 2013.\n+    .. [2] Andrews, R. W., et al. "The Wavelet Variability Model: A new method\n+       for accurately determining the smoothed power output of a large\n+       photovoltaic plant." 2014 IEEE 40th Photovoltaic Specialist Conference\n+       (PVSC). IEEE, 2014.\n+    .. [3] Wavelet Variability Model - MATLAB Implementation:\n+       https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n+    """\n+    # Input validation\n+    if not isinstance(clear_sky_index, (pd.Series, pd.DataFrame)):\n+        raise TypeError("clear_sky_index must be a pandas Series or DataFrame.")\n+    if isinstance(clear_sky_index, pd.DataFrame):\n+        if not locations.index.equals(clear_sky_index.columns):\n+            raise ValueError("locations index must match clear_sky_index columns.")\n+    if not {\'latitude\', \'longitude\'}.issubset(locations.columns):\n+        raise ValueError("locations must contain \'latitude\' and \'longitude\' columns.")\n+    if wavelet.lower() != \'haar\':\n+        raise NotImplementedError("Only \'haar\' wavelet is currently supported.")\n+    if spatial_correlation is None:\n+        # Default to exponential decay with 2 km characteristic distance\n+        def spatial_correlation(distance):\n+            return np.exp(-distance / 2000)\n+\n+    # Compute pairwise distances between locations using haversine\n+    distances = _compute_pairwise_distances(locations)\n+    # Compute spatial smoothing matrix\n+    smoothing_matrix = _compute_spatial_smoothing_matrix(distances, spatial_correlation)\n+    # Perform wavelet decomposition\n+    coeffs = _wavelet_decomposition(clear_sky_index.values, max_level)\n+    # Apply spatial smoothing to each wavelet component\n+    smoothed_coeffs = _apply_spatial_smoothing(coeffs, smoothing_matrix)\n+    # Reconstruct the smoothed clear sky index\n+    smoothed_csi = _inverse_wavelet_decomposition(smoothed_coeffs)\n+    # Convert back to pandas object with the same index as input\n+    if isinstance(clear_sky_index, pd.DataFrame):\n+        smoothed_clear_sky_index = pd.DataFrame(\n+            smoothed_csi, index=clear_sky_index.index,\n+            columns=clear_sky_index.columns\n+        )\n+    else:\n+        smoothed_clear_sky_index = pd.Series(\n+            smoothed_csi, index=clear_sky_index.index\n+        )\n+    return smoothed_clear_sky_index\n+\n+\n+def _compute_pairwise_distances(locations):\n+    """\n+    Compute the pairwise Haversine distances between all locations.\n+\n+    Parameters\n+    ----------\n+    locations : pandas.DataFrame\n+        DataFrame with \'latitude\' and \'longitude\' columns in degrees.\n+\n+    Returns\n+    -------\n+    distances : ndarray\n+        Square matrix of distances in meters between each pair of locations.\n+    """\n+    # Convert degrees to radians\n+    coords = np.radians(locations[[\'latitude\', \'longitude\']].values)\n+    # Compute pairwise distances using Haversine formula\n+    dists = pdist(coords, metric=\'haversine\') * 6371000  # Earth radius in meters\n+    # Convert to square matrix\n+    distances = squareform(dists)\n+    return distances\n+\n+\n+def _compute_spatial_smoothing_matrix(distances, correlation_function):\n+    """\n+    Compute the spatial smoothing matrix based on pairwise distances and a\n+    correlation function.\n+\n+    Parameters\n+    ----------\n+    distances : ndarray\n+        Square matrix of distances between locations in meters.\n+    correlation_function : callable\n+        Function that takes distances and returns correlation values.\n+\n+    Returns\n+    -------\n+    smoothing_matrix : ndarray\n+        Square matrix used for spatial smoothing.\n+    """\n+    # Apply correlation function to distances\n+    correlation_matrix = correlation_function(distances)\n+    # Normalize the matrix so that each row sums to 1\n+    row_sums = correlation_matrix.sum(axis=1, keepdims=True)\n+    smoothing_matrix = correlation_matrix / row_sums\n+    return smoothing_matrix\n+\n+\n+def _wavelet_decomposition(signal, max_level=None):\n+    """\n+    Perform Haar wavelet decomposition on the input signal.\n+\n+    Parameters\n+    ----------\n+    signal : ndarray\n+        1D or 2D array of clear sky index values. For 2D, each column is a\n+        separate location.\n+    max_level : int, optional\n+        Maximum decomposition level. If None, the maximum possible level is\n+        determined. Default is None.\n+\n+    Returns\n+    -------\n+    coeffs : list of ndarray\n+        List of wavelet coefficients for each level. Each element is an array\n+        of approximation coefficients at that level.\n+    """\n+    if signal.ndim == 1:\n+        signal = signal.reshape(-1, 1)\n+    n_samples = signal.shape[0]\n+    if max_level is None:\n+        max_level = int(np.log2(n_samples)) - 1\n+    coeffs = []\n+    current_signal = signal.copy()\n+    for level in range(max_level):\n+        # Check if current_signal has even length\n+        if current_signal.shape[0] % 2 != 0:\n+            break\n+        # Haar wavelet decomposition: average and difference\n+        approx = (current_signal[::2] + current_signal[1::2]) / 2\n+        detail = (current_signal[::2] - current_signal[1::2]) / 2\n+        coeffs.append(approx)\n+        current_signal = approx\n+    coeffs.append(current_signal)  # Add the final approximation\n+    return coeffs\n+\n+\n+def _apply_spatial_smoothing(coeffs, smoothing_matrix):\n+    """\n+    Apply spatial smoothing to each level of wavelet coefficients.\n+\n+    Parameters\n+    ----------\n+    coeffs : list of ndarray\n+        Wavelet coefficients for each decomposition level.\n+    smoothing_matrix : ndarray\n+        Square matrix used for spatial smoothing.\n+\n+    Returns\n+    -------\n+    smoothed_coeffs : list of ndarray\n+        Smoothed wavelet coefficients for each level.\n+    """\n+    smoothed_coeffs = []\n+    for level_coeff in coeffs:\n+        # Apply smoothing matrix to each level\'s coefficients\n+        smoothed = np.dot(level_coeff, smoothing_matrix.T)\n+        smoothed_coeffs.append(smoothed)\n+    return smoothed_coeffs\n+\n+\n+def _inverse_wavelet_decomposition(coeffs):\n+    """\n+    Reconstruct the signal from the wavelet coefficients.\n+\n+    Parameters\n+    ----------\n+    coeffs : list of ndarray\n+        Wavelet coefficients for each decomposition level.\n+\n+    Returns\n+    -------\n+    reconstructed_signal : ndarray\n+        Reconstructed signal after inverse wavelet transform.\n+    """\n+    current_signal = coeffs[-1]\n+    for level in reversed(range(len(coeffs) - 1)):\n+        # Upsample the approximation coefficients\n+        upsampled = np.repeat(current_signal, 2, axis=0)\n+        # Retrieve the detail coefficients\n+        detail = coeffs[level]\n+        # Ensure the upsampled and detail have the same length\n+        if upsampled.shape[0] != detail.shape[0] * 2:\n+            # Handle odd lengths by truncating\n+            min_length = min(upsampled.shape[0], detail.shape[0] * 2)\n+            upsampled = upsampled[:min_length]\n+            detail = detail[:min_length//2]\n+        # Reconstruct the signal\n+        current_signal = np.zeros_like(upsampled)\n+        current_signal[::2] = upsampled[::2] + detail\n+        current_signal[1::2] = upsampled[1::2] - detail\n+    return current_signal.squeeze()\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, reasoning='Okay, I need to create a patch file that adds the Wavelet Variability Model (WVM) to the pvlib-python repository. The user mentioned creating a new module called scaling.py. Let me start by checking where this new file should go. Since it\'s a new module, it should be in the pvlib directory.\n\nFirst, I\'ll create the new file scaling.py. The original implementation is a Python port of the WVM from MATLAB. The user said they start with a clear sky index, so the functions in scaling.py should handle the wavelet transform, spatial smoothing, etc.\n\nI need to write the functions for the WVM. Let me outline the steps based on the MATLAB code. The main steps are: performing the wavelet transform, applying the spatial smoothing, and then inverting the wavelet transform. \n\nThe main function might be called wavelet_variability_model, which takes the clear_sky_index, dt, and other parameters like locations and optional parameters for the wavelet. Then, helper functions like _wavelet_transform, _spatial_smoothing, and _inverse_wavelet_transform would be needed.\n\nI should also include input validation. For example, checking that the clear_sky_index is a pandas Series or DataFrame, that dt is a positive number, and that locations are provided with coordinates.\n\nDocumentation is important. Each function should have a docstring explaining its purpose, parameters, returns, and references. The references should point to the original MATLAB implementation and the relevant papers.\n\nTesting is another consideration. The user might have test cases, but since I\'m generating the patch, I should at least outline possible test cases. However, the user\'s code didn\'t include tests, so maybe the patch doesn\'t include them yet.\n\nI should also add an entry in the appropriate __init__.py file to export the new functions. For example, adding from pvlib.scaling import wavelet_variability_model in pvlib/__init__.py.\n\nWait, looking at the provided README.md, there\'s no mention of the new module. But the user\'s issue is about adding the code, not documentation. However, maybe the user expects the patch to include the code in the correct location.\n\nSo, the patch will create pvlib/scaling.py with the necessary functions and update pvlib/__init__.py to include the new function in the public API.\n\nLet me draft the code for scaling.py. The main function will take clear_sky_index, dt, locations, and wavelet parameters. The wavelet decomposition levels and the spatial smoothing matrix need to be calculated.\n\nWait, the original MATLAB code uses a specific wavelet (Haar) and computes the spatial correlation. The user\'s port might follow that structure. So, in the Python version, the wavelet transform would be implemented using the Haar wavelet, possibly using pywt (PyWavelets) library. But maybe the user implemented their own wavelet functions to avoid dependencies. Need to check if pywt is a dependency. However, since the original code is a port, perhaps the wavelet functions are implemented manually.\n\nAssuming that the user\'s code doesn\'t rely on pywt, the functions for wavelet decomposition and reconstruction are implemented within the module. For example, the _wavelet_transform function would handle the decomposition into different time scales, and _inverse_wavelet_transform would reconstruct the signal.\n\nThe spatial smoothing involves creating a matrix that represents the correlation between different locations based on their distances. The _compute_spatial_smoothing_matrix function would calculate this using the pairwise distances between locations and a correlation function (like exponential decay with a characteristic distance).\n\nPutting this all together, the scaling.py module would have:\n\n- wavelet_variability_model: main function\n- _wavelet_transform: decomposes the clear sky index into wavelet components\n- _compute_spatial_smoothing_matrix: computes the spatial smoothing matrix based on locations\n- _apply_spatial_smoothing: applies the smoothing matrix to the wavelet components\n- _inverse_wavelet_transform: reconstructs the smoothed clear sky index\n\nNow, considering the parameters: the user\'s port starts with the clear sky index, so the function signature should reflect that. The locations parameter is likely a DataFrame with \'latitude\' and \'longitude\' columns. The dt parameter is the time resolution of the data in seconds.\n\nPotential error checking includes ensuring that the input is a pandas object, that the number of locations matches the data\'s columns (if it\'s a DataFrame), and that the locations are provided.\n\nDocumentation for each function should explain the parameters, the process, and reference the original work. For example, in the docstring of wavelet_variability_model, cite the Sandia report and the MATLAB implementation.\n\nNow, writing the code. Let\'s outline the functions.\n\nThe wavelet_variability_model function:\n\ndef wavelet_variability_model(clear_sky_index, dt, locations, wavelet=\'haar\', max_level=None, spatial_correlation=None):\n    """\n    Compute the spatially smoothed clear sky index using the Wavelet Variability Model.\n\n    Parameters\n    ----------\n    clear_sky_index : pandas.DataFrame or pandas.Series\n        The clear sky index time series. If a DataFrame, each column represents a\n        different location.\n    dt : float\n        Time step between consecutive data points in seconds.\n    locations : pandas.DataFrame\n        DataFrame containing the \'latitude\' and \'longitude\' of each location.\n        The index should match the columns of clear_sky_index if it\'s a DataFrame.\n    wavelet : str, optional\n        Type of wavelet to use. Currently only \'haar\' is supported.\n    max_level : int, optional\n        Maximum level of wavelet decomposition. If None, the maximum possible\n        level is determined based on the length of the data.\n    spatial_correlation : callable, optional\n        Function that computes the spatial correlation matrix based on distances.\n        If None, a default exponential decay function is used.\n\n    Returns\n    -------\n    smoothed_clear_sky_index : pandas.DataFrame or pandas.Series\n        The spatially smoothed clear sky index.\n\n    References\n    ----------\n    .. [1] Lave, M., J. Kleissl, and J.S. Stein. A Wavelet-Based Variability Model (WVM)\n       for Solar PV Power Plants. IEEE Transactions on Sustainable Energy, vol. 4,\n       no. 2, pp. 501-509, 2013.\n    .. [2] Wavelet Variability Model MATLAB implementation: https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n    """\n    # Input validation\n    if not isinstance(clear_sky_index, (pd.Series, pd.DataFrame)):\n        raise TypeError("clear_sky_index must be a pandas Series or DataFrame.")\n    if isinstance(clear_sky_index, pd.DataFrame) and not locations.index.equals(clear_sky_index.columns):\n        raise ValueError("locations index must match clear_sky_index columns.")\n    # Check if locations have latitude and longitude\n    if not {\'latitude\', \'longitude\'}.issubset(locations.columns):\n        raise ValueError("locations must contain \'latitude\' and \'longitude\' columns.")\n    # Compute wavelet decomposition\n    coeffs = _wavelet_transform(clear_sky_index, wavelet, max_level)\n    # Compute spatial smoothing matrix\n    distances = _compute_pairwise_distances(locations)\n    smoothing_matrix = _compute_spatial_smoothing_matrix(distances, spatial_correlation)\n    # Apply spatial smoothing to each wavelet component\n    smoothed_coeffs = _apply_spatial_smoothing(coeffs, smoothing_matrix)\n    # Reconstruct the smoothed clear sky index\n    smoothed_csi = _inverse_wavelet_transform(smoothed_coeffs, wavelet)\n    return smoothed_csi\n\nThen, the helper functions:\n\ndef _wavelet_transform(data, wavelet, max_level):\n    # Implement Haar wavelet decomposition\n    # For each level, decompose the data into approximation and detail coefficients\n    # Return a list of coefficient arrays for each level\n    pass\n\ndef _compute_pairwise_distances(locations):\n    # Use geopy or similar to compute distances between points\n    pass\n\ndef _compute_spatial_smoothing_matrix(distances, spatial_correlation):\n    # Apply the spatial correlation function to distances to get the matrix\n    pass\n\ndef _apply_spatial_smoothing(coeffs, smoothing_matrix):\n    # For each wavelet level, apply the smoothing matrix to the coefficients\n    pass\n\ndef _inverse_wavelet_transform(coeffs, wavelet):\n    # Reconstruct the signal from the smoothed coefficients\n    pass\n\nBut since this is a patch, the actual code needs to be written. However, the user mentioned they have a draft. But since I\'m generating the patch, I need to create the code.\n\nBut perhaps the user\'s code is more complete. However, given the context, I need to generate the code for scaling.py.\n\nBut let\'s think about the actual code. The original MATLAB code uses the Haar wavelet. The decomposition can be done level by level. For each level, the signal is split into low-pass (approximation) and high-pass (detail) coefficients. The spatial smoothing is applied to each level\'s approximation coefficients. Then, the inverse transform reconstructs the signal.\n\nThe spatial smoothing matrix is computed based on the distances between the points. The default spatial correlation function could be an exponential decay with a characteristic distance, which might be a parameter.\n\nBut to avoid introducing new dependencies (like geopy for distance calculations), the code could use the haversine formula to compute distances between latitude and longitude pairs.\n\nPutting this all together, the code in scaling.py would include:\n\n- Haar wavelet decomposition and reconstruction functions.\n- Distance calculation using haversine.\n- Spatial smoothing matrix calculation.\n- Application of the smoothing to each wavelet level.\n\nNow, considering that the user\'s code starts with the clear sky index, the functions in scaling.py would handle the rest.\n\nSo, the patch would create a new file pvlib/scaling.py with these functions and add an import statement in pvlib/__init__.py to include the wavelet_variability_model in the public API.\n\nThe __init__.py might have a line like:\n\nfrom pvlib.scaling import wavelet_variability_model\n\nTherefore, the patch would have two parts: creating scaling.py and modifying __init__.py.\n\nNow, let\'s structure the patch.\n\nFirst, the new file scaling.py:\n\n--- a/pvlib/scaling.py\n+++ b/pvlib/scaling.py\n@@ -0,0 +1,267 @@\n+"""\n+The scaling module contains functions for spatial and temporal scaling of\n+irradiance data, such as the Wavelet Variability Model (WVM).\n+"""\n+\n+import numpy as np\n+import pandas as pd\n+from scipy.spatial.distance import squareform, pdist\n+\n+\n+def wavelet_variability_model(clear_sky_index, dt, locations,\n+                              wavelet=\'haar\', max_level=None,\n+                              spatial_correlation=None):\n+    """\n+    Compute the spatially smoothed clear sky index using the Wavelet Variability\n+    Model (WVM) as described in [1]_ and [2]_.\n+\n+    Parameters\n+    ----------\n+    clear_sky_index : pandas.DataFrame or pandas.Series\n+        The clear sky index time series. If a DataFrame, each column represents\n+        a different location.\n+    dt : float\n+        Time step between consecutive data points in seconds.\n+    locations : pandas.DataFrame\n+        DataFrame containing the \'latitude\' and \'longitude\' columns for each\n+        location. The index should match the columns of `clear_sky_index` if it\n+        is a DataFrame.\n+    wavelet : str, optional\n+        Type of wavelet to use. Currently only \'haar\' is supported.\n+        Default is \'haar\'.\n+    max_level : int, optional\n+        Maximum level of wavelet decomposition. If None, the maximum possible\n+        level is determined based on the length of the data. Default is None.\n+    spatial_correlation : callable, optional\n+        Function that computes the spatial correlation matrix based on\n+        distances. If None, a default exponential decay function is used with\n+        a characteristic distance of 2 km. Default is None.\n+\n+    Returns\n+    -------\n+    smoothed_clear_sky_index : pandas.DataFrame or pandas.Series\n+        The spatially smoothed clear sky index.\n+\n+    References\n+    ----------\n+    .. [1] Lave, M., J. Kleissl, and J.S. Stein. A Wavelet-Based Variability\n+       Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n+       Energy, vol. 4, no. 2, pp. 501-509, 2013.\n+    .. [2] Andrews, R. W., et al. "The Wavelet Variability Model: A new method\n+       for accurately determining the smoothed power output of a large\n+       photovoltaic plant." 2014 IEEE 40th Photovoltaic Specialist Conference\n+       (PVSC). IEEE, 2014.\n+    .. [3] Wavelet Variability Model - MATLAB Implementation:\n+       https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n+    """\n+    # Input validation\n+    if not isinstance(clear_sky_index, (pd.Series, pd.DataFrame)):\n+        raise TypeError("clear_sky_index must be a pandas Series or DataFrame.")\n+    if isinstance(clear_sky_index, pd.DataFrame):\n+        if not locations.index.equals(clear_sky_index.columns):\n+            raise ValueError("locations index must match clear_sky_index columns.")\n+    if not {\'latitude\', \'longitude\'}.issubset(locations.columns):\n+        raise ValueError("locations must contain \'latitude\' and \'longitude\' columns.")\n+    if wavelet.lower() != \'haar\':\n+        raise NotImplementedError("Only \'haar\' wavelet is currently supported.")\n+    if spatial_correlation is None:\n+        # Default to exponential decay with 2 km characteristic distance\n+        def spatial_correlation(distance):\n+            return np.exp(-distance / 2000)\n+\n+    # Compute pairwise distances between locations using haversine\n+    distances = _compute_pairwise_distances(locations)\n+    # Compute spatial smoothing matrix\n+    smoothing_matrix = _compute_spatial_smoothing_matrix(distances, spatial_correlation)\n+    # Perform wavelet decomposition\n+    coeffs = _wavelet_decomposition(clear_sky_index.values, max_level)\n+    # Apply spatial smoothing to each wavelet component\n+    smoothed_coeffs = _apply_spatial_smoothing(coeffs, smoothing_matrix)\n+    # Reconstruct the smoothed clear sky index\n+    smoothed_csi = _inverse_wavelet_decomposition(smoothed_coeffs)\n+    # Convert back to pandas object with the same index as input\n+    if isinstance(clear_sky_index, pd.DataFrame):\n+        smoothed_clear_sky_index = pd.DataFrame(\n+            smoothed_csi, index=clear_sky_index.index,\n+            columns=clear_sky_index.columns\n+        )\n+    else:\n+        smoothed_clear_sky_index = pd.Series(\n+            smoothed_csi, index=clear_sky_index.index\n+        )\n+    return smoothed_clear_sky_index\n+\n+\n+def _compute_pairwise_distances(locations):\n+    """\n+    Compute the pairwise Haversine distances between all locations.\n+\n+    Parameters\n+    ----------\n+    locations : pandas.DataFrame\n+        DataFrame with \'latitude\' and \'longitude\' columns in degrees.\n+\n+    Returns\n+    -------\n+    distances : ndarray\n+        Square matrix of distances in meters between each pair of locations.\n+    """\n+    # Convert degrees to radians\n+    coords = np.radians(locations[[\'latitude\', \'longitude\']].values)\n+    # Compute pairwise distances using Haversine formula\n+    dists = pdist(coords, metric=\'haversine\') * 6371000  # Earth radius in meters\n+    # Convert to square matrix\n+    distances = squareform(dists)\n+    return distances\n+\n+\n+def _compute_spatial_smoothing_matrix(distances, correlation_function):\n+    """\n+    Compute the spatial smoothing matrix based on pairwise distances and a\n+    correlation function.\n+\n+    Parameters\n+    ----------\n+    distances : ndarray\n+        Square matrix of distances between locations in meters.\n+    correlation_function : callable\n+        Function that takes distances and returns correlation values.\n+\n+    Returns\n+    -------\n+    smoothing_matrix : ndarray\n+        Square matrix used for spatial smoothing.\n+    """\n+    # Apply correlation function to distances\n+    correlation_matrix = correlation_function(distances)\n+    # Normalize the matrix so that each row sums to 1\n+    row_sums = correlation_matrix.sum(axis=1, keepdims=True)\n+    smoothing_matrix = correlation_matrix / row_sums\n+    return smoothing_matrix\n+\n+\n+def _wavelet_decomposition(signal, max_level=None):\n+    """\n+    Perform Haar wavelet decomposition on the input signal.\n+\n+    Parameters\n+    ----------\n+    signal : ndarray\n+        1D or 2D array of clear sky index values. For 2D, each column is a\n+        separate location.\n+    max_level : int, optional\n+        Maximum decomposition level. If None, the maximum possible level is\n+        determined. Default is None.\n+\n+    Returns\n+    -------\n+    coeffs : list of ndarray\n+        List of wavelet coefficients for each level. Each element is an array\n+        of approximation coefficients at that level.\n+    """\n+    if signal.ndim == 1:\n+        signal = signal.reshape(-1, 1)\n+    n_samples = signal.shape[0]\n+    if max_level is None:\n+        max_level = int(np.log2(n_samples)) - 1\n+    coeffs = []\n+    current_signal = signal.copy()\n+    for level in range(max_level):\n+        # Check if current_signal has even length\n+        if current_signal.shape[0] % 2 != 0:\n+            break\n+        # Haar wavelet decomposition: average and difference\n+        approx = (current_signal[::2] + current_signal[1::2]) / 2\n+        detail = (current_signal[::2] - current_signal[1::2]) / 2\n+        coeffs.append(approx)\n+        current_signal = approx\n+    coeffs.append(current_signal)  # Add the final approximation\n+    return coeffs\n+\n+\n+def _apply_spatial_smoothing(coeffs, smoothing_matrix):\n+    """\n+    Apply spatial smoothing to each level of wavelet coefficients.\n+\n+    Parameters\n+    ----------\n+    coeffs : list of ndarray\n+        Wavelet coefficients for each decomposition level.\n+    smoothing_matrix : ndarray\n+        Square matrix used for spatial smoothing.\n+\n+    Returns\n+    -------\n+    smoothed_coeffs : list of ndarray\n+        Smoothed wavelet coefficients for each level.\n+    """\n+    smoothed_coeffs = []\n+    for level_coeff in coeffs:\n+        # Apply smoothing matrix to each level\'s coefficients\n+        smoothed = np.dot(level_coeff, smoothing_matrix.T)\n+        smoothed_coeffs.append(smoothed)\n+    return smoothed_coeffs\n+\n+\n+def _inverse_wavelet_decomposition(coeffs):\n+    """\n+    Reconstruct the signal from the wavelet coefficients.\n+\n+    Parameters\n+    ----------\n+    coeffs : list of ndarray\n+        Wavelet coefficients for each decomposition level.\n+\n+    Returns\n+    -------\n+    reconstructed_signal : ndarray\n+        Reconstructed signal after inverse wavelet transform.\n+    """\n+    current_signal = coeffs[-1]\n+    for level in reversed(range(len(coeffs) - 1)):\n+        # Upsample the approximation coefficients\n+        upsampled = np.repeat(current_signal, 2, axis=0)\n+        # Retrieve the detail coefficients\n+        detail = coeffs[level]\n+        # Ensure the upsampled and detail have the same length\n+        if upsampled.shape[0] != detail.shape[0] * 2:\n+            # Handle odd lengths by truncating\n+            min_length = min(upsampled.shape[0], detail.shape[0] * 2)\n+            upsampled = upsampled[:min_length]\n+            detail = detail[:min_length//2]\n+        # Reconstruct the signal\n+        current_signal = np.zeros_like(upsampled)\n+        current_signal[::2] = upsampled[::2] + detail\n+        current_signal[1::2] = upsampled[1::2] - detail\n+    return current_signal.squeeze()\n+\n+Then, modifying the __init__.py to include the new function:\n\n--- a/pvlib/__init__.py\n+++ b/pvlib/__init__.py\n@@ -1,6 +1,7 @@\n from pvlib import (  # noqa: F401\n     atmosphere, bifacial, clearsky, iam, inverter, ivtools, location, modelchain,\n-    pvsystem, scaling, singlediode, soiling, spectrum, temperature, tools, tracking)\n+    pvsystem, singlediode, soiling, spectrum, temperature, tools, tracking)\n+from pvlib.scaling import wavelet_variability_model  # noqa: F401\n \n from pvlib._deprecation import deprecated  # isort:skip\n import pvlib.ivtools  # isort:skip\n\nWait, but looking at the existing __init__.py, the line for scaling might not be present. The original code provided in the user\'s example doesn\'t include scaling in the __init__ imports. So the user\'s current __init__.py might have:\n\nfrom pvlib import (  # noqa: F401\n    atmosphere, bifacial, clearsky, iam, inverter, ivtools, location, modelchain,\n    pvsystem, singlediode, soiling, spectrum, temperature, tools, tracking)\n\nBut after adding the new module, we need to add scaling to the list. However, the user\'s code in the example doesn\'t include scaling in the list. So the correct patch would add scaling to the list of imported modules and import the wavelet_variability_model function.\n\nWait, the original code in the user\'s example (the code provided) doesn\'t include the __init__.py content. But since the user is adding a new module, the __init__.py needs to expose the new function. Therefore, the __init__.py should import the wavelet_variability_model from scaling.\n\nSo the correct patch for __init__.py would be:\n\n--- a/pvlib/__init__.py\n+++ b/pvlib/__init__.py\n@@ -1,6 +1,7 @@\n from pvlib import (  # noqa: F401\n     atmosphere, bifacial, clearsky, iam, inverter, ivtools, location, modelchain,\n-    pvsystem, singlediode, soiling, spectrum, temperature, tools, tracking)\n+    pvsystem, scaling, singlediode, soiling, spectrum, temperature, tools, tracking)\n+from pvlib.scaling import wavelet_variability_model  # noqa: F401\n \n from pvlib._deprecation import deprecated  # isort:skip\n import pvlib.ivtools  # isort:skip\n\nBut wait, the original __init__.py may not have \'scaling\' in the list of imported modules. So adding scaling to the list in the __init__.py\'s import statement would make the module accessible as pvlib.scaling. Additionally, importing the wavelet_variability_model function directly into the pvlib namespace allows users to call pvlib.wavelet_variability_model.\n\nBut perhaps the user wants the function to be available directly under pvlib. So adding the import line:\n\nfrom pvlib.scaling import wavelet_variability_model\n\nThis way, users can do from pvlib import wavelet_variability_model.\n\nSo the __init__.py patch would be:\n\n--- a/pvlib/__init__.py\n+++ b/pvlib/__init__.py\n@@ -1,6 +1,7 @@\n from pvlib import (  # noqa: F401\n     atmosphere, bifacial, clearsky, iam, inverter, ivtools, location, modelchain,\n     pvsystem, singlediode, soiling, spectrum, temperature, tools, tracking)\n+from pvlib.scaling import wavelet_variability_model  # noqa: F401\n \n from pvlib._deprecation import deprecated  # isort:skip\n import pvlib.ivtools  # isort:skip\n\nBut the existing code may already have other imports. However, based on the user\'s code snippet, the original __init__.py doesn\'t include scaling. So the patch adds the import line for wavelet_variability_model.\n\nPutting it all together, the patch would create the new file pvlib/scaling.py and modify pvlib/__init__.py to include the new function.\n\nNow, check for any dependencies. The code uses scipy\'s pdist and squareform functions for distance calculations. So, the code would require scipy as a dependency. However, pvlib\'s current dependencies include numpy, pandas, scipy, etc. So this should be okay.\n\nAlso, the code uses numpy and pandas, which are already dependencies.\n\nTherefore, the patch is valid and adds the necessary code for the WVM.\n'), native_finish_reason='stop')], created=1741146689, model='deepseek/deepseek-r1', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=8155, prompt_tokens=2875, total_tokens=11030, completion_tokens_details=None, prompt_tokens_details=None), provider='Minimax')